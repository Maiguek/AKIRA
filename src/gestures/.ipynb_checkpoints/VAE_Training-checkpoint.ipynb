{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493b9ad8-3fee-46ee-aedd-ce4b98cef591",
   "metadata": {},
   "source": [
    "# Testing Area for Gesture Generation - VRAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ababfc-3fa5-4dac-9425-0692f0200ecd",
   "metadata": {},
   "source": [
    "## VRAE for Arm Gesture Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f066b-2fe7-4931-8ebd-4e3d007093a2",
   "metadata": {},
   "source": [
    "Plan to execute:\n",
    "\n",
    "1. Data Collection. (Record me talking and moving my arms, then split it in around 30 seconds (better if I check exactly how much text Akira produces in one batch so I can mimic the timings), clean it, normalize it, and scale it for training.\n",
    "2. Define and train VRAE model to learn the latent space, and later be able to be finetuned to match movements with text.\n",
    "3. Make sure the output is similar to the one used in the inverse kynematics one to be able later to use it for respective movement generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1f283-2395-4a9d-a6e4-152ac20f0ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
